#+TITLE: assignments_04

* 说明

使用 Gensim 和维基百科获得你的第一批词向量，并感受词向量的基本过程。

[[file:assignments_04/001.png]]

* 第一步：下载中文维基词汇
[[https://dumps.wikimedia.org/zhwiki/20190720/][维基中文词汇]]

* 第二步：使用 =wikiextractor= 解压
[[https://github.com/attardi/wikiextractor][wikipedia extractor]]

* 第三步：使用 =gensim= 获取词向量

** 参考网址

[[https://radimrehurek.com/gensim/models/word2vec.html]]
[[https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne]]

** 注意事项

1. 使用 =Jieba= 分词将维基百科内容切分
2. 使用 =Gensim= 的 =LineSentence= 这个类进行文件的读取

** 实现步骤

1. 路径循环，文件分别处理
2. 将类似 "<*>" 标签处理掉，将换行符等处理掉
3. 繁体，简体转换
4. 导入 =gensim= 的 =word2vec= 模型并训练

   #+BEGIN_SRC python
import os
import hanziconv
import re
import jieba
from gensim.models import word2vec

root_dir = "text/"

# 正则匹配定义，去除 <*>
p = re.compile("<[^>]+>")

total_words = []

for directory in os.listdir(root_dir):
    for extract_file in os.listdir(root_dir+os.sep+directory):
        origin_text = [hanziconv.HanziConv.toSimplified(p.sub("", text.strip().rstrip())) for text in open(root_dir+os.sep+directory+os.sep+extract_file)]
        for text in origin_text:
            if not text:
                continue
            total_words.append(list(jieba.cut(text)))

word2vec = word2vec.Word2Vec(corpus, size=100, window=20, min_count=200, workers=4)
   #+END_SRC
* 第四步：测试几个词测试模型表现


#+BEGIN_SRC python
In [16]: model.wv.most_similar("工人")
Out[16]:
[('女工', 0.733999490737915),
 ('矿工', 0.7052727341651917),
 ('职工', 0.7050555348396301),
 ('建筑工人', 0.6886410117149353),
 ('工会', 0.6765564680099487),
 ('劳动', 0.6699349284172058),
 ('铁路工人', 0.6642400622367859),
 ('农民', 0.647709846496582),
 ('劳工', 0.6393541097640991),
 ('工人运动', 0.6309130787849426)]

In [17]: model.wv.most_similar("我")
Out[17]:
[('你', 0.9054259657859802),
 ('我们', 0.7964417338371277),
 ('你们', 0.7655521035194397),
 ('我要', 0.7535097599029541),
 ('真的', 0.7373670339584351),
 ('妳', 0.733460545539856),
 ('说', 0.7247865200042725),
 ('啊', 0.7190474271774292),
 ('！', 0.7132689952850342),
 ('对不起', 0.7040584683418274)]

In [18]: model.wv.most_similar("女人")
Out[18]:
[('男人', 0.9506567120552063),
 ('女孩', 0.7366923093795776),
 ('情人', 0.6872472763061523),
 ('处女', 0.6834422945976257),
 ('她们', 0.682411789894104),
 ('爱情', 0.6719450950622559),
 ('陌生人', 0.6643493175506592),
 ('女孩子', 0.6632941961288452),
 ('寡妇', 0.6632471084594727),
 ('温柔', 0.6619871258735657)]
#+END_SRC
* 第五步：使用可视化工具

#+BEGIN_SRC python
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

def tsne_plot(model):
    "Creates and TSNE model and plots it"
    labels = []
    tokens = []

    for word in model.wv.vocab:
        tokens.append(model[word])
        labels.append(word)

    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)
    new_values = tsne_model.fit_transform(tokens)

    x = []
    y = []
    for value in new_values:
        x.append(value[0])
        y.append(value[1])

    plt.figure(figsize=(16, 16))
    for i in range(len(x)):
        plt.scatter(x[i],y[i])
        plt.annotate(labels[i],
                     xy=(x[i], y[i]),
                     xytext=(5, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.show()

tsne_plot(model)
#+END_SRC
